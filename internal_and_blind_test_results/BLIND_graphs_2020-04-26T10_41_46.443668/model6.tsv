# MLPRegressor
# dataset: whole_dataset
# date: 2020-04-26T10_41_46.446296
# parameters: {"hidden_layer_sizes": [50, 50], "alpha": 0.0009044205421256557, "batch_size": 1, "activation": "logistic", "learning_rate": "adaptive", "learning_rate_init": 0.06205633095090332, "power_t": 0.5, "max_iter": 200, "shuffle": true, "random_state": 42, "tol": 0.0001, "verbose": false, "warm_start": false, "momentum": 0.01510303444629194, "nesterovs_momentum": true, "early_stopping": false, "validation_fraction": 0.1, "n_iter_no_change": 10, "weights_init_fun": "random_normal", "weights_init_value": 0.7}
epoch	train_loss(squared)	valid_loss(squared)	valid_accuracy(euclidean)	train_accuracy(euclidean)
1	2.40593204709638	2.40593204709638	1.8682831901515775	1.8682831901515775
2	1.9106785302118532	1.9106785302118532	1.6513457318729485	1.6513457318729485
3	2.126568482734981	2.126568482734981	1.657935136268112	1.657935136268112
4	2.021475532020376	2.021475532020376	1.5394568811478573	1.5394568811478573
5	1.3119636105244543	1.3119636105244543	1.2965648828131555	1.2965648828131555
6	1.2041977059094058	1.2041977059094058	1.2665883859994165	1.2665883859994165
7	1.151540863477373	1.151540863477373	1.1804410042857272	1.1804410042857272
8	0.9519745417913665	0.9519745417913665	1.0845257128058012	1.0845257128058012
9	0.8583858815831029	0.8583858815831029	1.0279206009133348	1.0279206009133348
10	1.0325808509979064	1.0325808509979064	1.100808464681972	1.100808464681972
11	0.8777788010389583	0.8777788010389583	1.0227311196679854	1.0227311196679854
12	0.7642197624454837	0.7642197624454837	0.9449533266812686	0.9449533266812686
13	0.9016264647977321	0.9016264647977321	1.0824370059801693	1.0824370059801693
14	0.7530661525644345	0.7530661525644345	0.9455437962408527	0.9455437962408527
15	0.7626412359272435	0.7626412359272435	0.9510565208858803	0.9510565208858803
16	0.8276693369255178	0.8276693369255178	1.037255231460543	1.037255231460543
17	0.7555839946487669	0.7555839946487669	0.9662158497485196	0.9662158497485196
18	0.7756713482166527	0.7756713482166527	0.979465861866552	0.979465861866552
19	0.6966302576345799	0.6966302576345799	0.9139588989988534	0.9139588989988534
20	0.6810372621955454	0.6810372621955454	0.8987074471362522	0.8987074471362522
21	0.7010729503910638	0.7010729503910638	0.9077852372061325	0.9077852372061325
22	0.6665229255091554	0.6665229255091554	0.8918327442440583	0.8918327442440583
23	0.675085819037554	0.675085819037554	0.9011636471600298	0.9011636471600298
24	0.6676581191806841	0.6676581191806841	0.8927681338976373	0.8927681338976373
25	0.6576238191127348	0.6576238191127348	0.8861749086749501	0.8861749086749501
26	0.6517872198641645	0.6517872198641645	0.8802492270625017	0.8802492270625017
27	0.6490353350951851	0.6490353350951851	0.8791590387319455	0.8791590387319455
28	0.6556841898083617	0.6556841898083617	0.8868501488928164	0.8868501488928164
29	0.6558073647457797	0.6558073647457797	0.8842698874667355	0.8842698874667355
30	0.6476855515007223	0.6476855515007223	0.8791887894353246	0.8791887894353246
31	0.6492601683080095	0.6492601683080095	0.8791798700376734	0.8791798700376734
32	0.6494534603976978	0.6494534603976978	0.8790518160160294	0.8790518160160294
33	0.6448339458641099	0.6448339458641099	0.8758416565671863	0.8758416565671863
34	0.6457740169436768	0.6457740169436768	0.8779745492900992	0.8779745492900992
35	0.6448049838883514	0.6448049838883514	0.8755324220139811	0.8755324220139811
36	0.6450414046606477	0.6450414046606477	0.8762949330253326	0.8762949330253326
37	0.6450247825578556	0.6450247825578556	0.875900531696935	0.875900531696935
38	0.6458129662241369	0.6458129662241369	0.876609457338579	0.876609457338579
39	0.6461435579911262	0.6461435579911262	0.87683034474375	0.87683034474375
40	0.6471006407370755	0.6471006407370755	0.8776435626693057	0.8776435626693057
41	0.6478132495997877	0.6478132495997877	0.8782949722902844	0.8782949722902844
42	0.6493451560224051	0.6493451560224051	0.879966827083483	0.879966827083483
43	0.6507828271771411	0.6507828271771411	0.8815944304043699	0.8815944304043699
