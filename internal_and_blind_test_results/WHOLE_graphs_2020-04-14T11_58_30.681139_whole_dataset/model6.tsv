# MLPRegressor
# dataset: whole_dataset
# date: 2020-04-14T11_58_30.682393
# parameters: {"hidden_layer_sizes": [50, 50], "alpha": 0.0009044205421256557, "batch_size": 1, "activation": "logistic", "learning_rate": "adaptive", "learning_rate_init": 0.06205633095090332, "power_t": 0.5, "max_iter": 200, "shuffle": true, "random_state": 42, "tol": 0.0001, "verbose": false, "warm_start": false, "momentum": 0.01510303444629194, "nesterovs_momentum": true, "early_stopping": false, "validation_fraction": 0.1, "n_iter_no_change": 10, "weights_init_fun": "random_normal", "weights_init_value": 0.7}
epoch	train_loss(squared)	valid_loss(squared)	valid_accuracy(euclidean)
1	2.6220357915934995	2.6220357915934995	1.960708893559126
2	1.87114305392831	1.87114305392831	1.6319932024104065
3	1.3239249312404997	1.3239249312404997	1.3026287032939021
4	1.5261431925631896	1.5261431925631896	1.4210158975644354
5	1.7218721984291365	1.7218721984291365	1.4313882706590488
6	0.9367395898955855	0.9367395898955855	1.05845763253932
7	1.2034409950981901	1.2034409950981901	1.2246075094058397
8	0.87967521188069	0.87967521188069	1.0422669869603123
9	1.1170027789812644	1.1170027789812644	1.1410961011231715
10	0.9146278417709823	0.9146278417709823	1.081008880183105
11	0.821485265482634	0.821485265482634	1.0095723262037153
12	0.8405492021451136	0.8405492021451136	1.0027758596838487
13	0.768048423140774	0.768048423140774	0.9473729579607383
14	0.797378212652267	0.797378212652267	0.9891527963739442
15	0.746139535998569	0.746139535998569	0.9529112534083103
16	0.786745242800554	0.786745242800554	0.9933023175887835
17	0.7104095004529837	0.7104095004529837	0.8965868909727888
18	0.7119529337937658	0.7119529337937658	0.9305255691698554
19	0.7577537116701742	0.7577537116701742	0.9603043980872027
20	0.7064303976828619	0.7064303976828619	0.9189571830956138
21	0.6436163340922272	0.6436163340922272	0.8663090286679006
22	0.6444274531948274	0.6444274531948274	0.8643104391213837
23	0.6684124828011777	0.6684124828011777	0.8837427649942423
24	0.6308065671473425	0.6308065671473425	0.857272504839736
25	0.6403648336641948	0.6403648336641948	0.8680452781141517
26	0.6210591176509215	0.6210591176509215	0.8515914050679382
27	0.6232605466695501	0.6232605466695501	0.8539596358883262
28	0.6360934402544112	0.6360934402544112	0.8686300393436271
29	0.6181551092893263	0.6181551092893263	0.8491094041859637
30	0.6152657278875042	0.6152657278875042	0.8495352678733225
31	0.6117266557329307	0.6117266557329307	0.8452878985458702
32	0.6119938195435628	0.6119938195435628	0.8463535510211027
33	0.6153975907710396	0.6153975907710396	0.8473493112131439
34	0.6086901409089037	0.6086901409089037	0.8419676956246577
35	0.6117271664075399	0.6117271664075399	0.8480766556753829
36	0.6103371374726225	0.6103371374726225	0.8443746364528497
37	0.6093943589573446	0.6093943589573446	0.8444423341246552
38	0.6081420127889989	0.6081420127889989	0.8430007867620622
39	0.6087633582592286	0.6087633582592286	0.8431482707449084
40	0.6105686355656047	0.6105686355656047	0.846043185827411
41	0.6087663112850185	0.6087663112850185	0.8435371007331568
42	0.6088335376416381	0.6088335376416381	0.8439227312708725
43	0.6095108054594237	0.6095108054594237	0.8449130173234348
44	0.6099366055921157	0.6099366055921157	0.8453045010172916
45	0.6109740917328356	0.6109740917328356	0.8467965207721677
46	0.6118881975569309	0.6118881975569309	0.8481928944354928
47	0.6134462636109376	0.6134462636109376	0.8505397281467878
48	0.6149860838176381	0.6149860838176381	0.8528497236783769
