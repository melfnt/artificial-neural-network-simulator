# MLPRegressor
# dataset: internal_test_set
# date: 2020-04-25T19_55_06.623912
# parameters: {"hidden_layer_sizes": [50, 50], "alpha": 0.0002188148237404819, "batch_size": 1, "activation": "logistic", "learning_rate": "constant", "learning_rate_init": 0.016437178740396977, "power_t": 0.5, "max_iter": 200, "shuffle": true, "random_state": 42, "tol": 0.0001, "verbose": false, "warm_start": false, "momentum": 0.483140617180181, "nesterovs_momentum": true, "early_stopping": false, "validation_fraction": 0.1, "n_iter_no_change": 10, "weights_init_fun": "random_normal", "weights_init_value": 0.7}
epoch	train_loss(squared)	valid_loss(squared)	valid_accuracy(euclidean)	train_accuracy(euclidean)
1	1.3424960373874346	1.1864776972938975	1.2469506681753297	1.3081424036643263
2	1.344379819702459	1.2469441500753806	1.2823258356547629	1.286914328263628
3	1.2516255294303527	1.2796711010568254	1.2815154005681118	1.2566223154238887
4	1.473534924091964	1.4736527550594178	1.3593103882797528	1.386917919154893
5	1.0528321076677183	1.0548835667551766	1.1339972204830595	1.1272688399019477
6	1.1182051402983353	1.0963874275975267	1.1389581527662702	1.1739329284672835
7	1.0682650875305437	1.1082699757793923	1.1468522808796495	1.1177180225081984
8	1.0755586275931135	1.103059951645445	1.1180508208165603	1.1174024758180068
9	1.5136499988672671	1.4316577999793427	1.2561076457809444	1.3107955462833236
10	1.0839506915270782	1.1547578488843104	1.1627273224686354	1.143373165321711
11	1.36684494231001	1.3919483773739203	1.3180622115491631	1.3020865573586642
12	0.913419893198701	0.9870655125109474	1.0507344629732016	1.0472871578987568
13	1.0608386431684476	1.0736441750010246	1.1281319547208264	1.134276778417879
14	2.0697700430840045	2.057555118143289	1.7395074816646425	1.7233235440162402
15	0.9998747854177984	1.1094056210743684	1.2141834617965035	1.1419185212298362
16	0.9390416794045746	1.0364906193743024	1.136438451240097	1.0765249417139005
17	0.944882741974517	1.032144373322317	1.153879741059227	1.0875807975513856
18	0.8095059087865089	0.9362747387276142	1.0514696350039259	0.9848004931048354
19	0.8489145910330401	1.0018650995617038	1.1320982772821606	1.0446724791694384
20	0.7781096881897186	0.9015119647483413	1.044791075765463	0.9652899845243866
21	0.8415919896685727	0.9819284111371865	1.0522192728437803	0.9965803456096037
22	0.9520412334698372	1.1230836161835436	1.1195345429394312	1.0655398965560976
23	0.8466915639084114	1.0074694783276066	1.098303919404246	1.0248025424885092
24	1.046436543402372	1.1538146295427552	1.2142779552753746	1.202784271364778
25	0.7788770381250586	0.9812513299330907	1.0842164829000849	0.9595876260019849
26	0.7432131951785927	0.9172718901167867	1.0315026532317704	0.9584028161606223
27	0.8281729245077017	0.9734595787005081	1.0949732511969101	1.0391007005560864
28	0.7680422546246628	0.9617812104513921	1.0526788285539461	0.9659476579632528
29	0.8783923864214315	1.0385054299953353	1.16134165867677	1.11419496763442
30	0.7686403365323417	0.9871097396286943	1.0767714967369053	0.9562107215032193
31	0.884894139371844	1.057746766799223	1.1953219004474942	1.0920551418859887
32	0.8089740866465376	0.9893563301158419	1.1164002009980036	0.9921155669161522
33	0.7491772446935543	0.9509969605834803	1.0636475360299478	0.9671093512418251
34	0.7761096052031139	1.0148684216743071	1.0987941840109328	0.991380155540295
35	0.7441966233560108	0.9806880012262288	1.0984223353592901	0.981995829580187
36	0.9399570959998185	1.1971345468607217	1.1847698596247465	1.0606276447999448
