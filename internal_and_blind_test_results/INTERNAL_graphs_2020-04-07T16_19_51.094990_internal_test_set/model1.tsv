# MLPRegressor
# dataset: internal_test_set
# date: 2020-04-07T16_19_51.096006
# parameters: {"hidden_layer_sizes": [50, 50], "alpha": 0.0005951915361345095, "batch_size": 1, "activation": "logistic", "learning_rate": "adaptive", "learning_rate_init": 0.054980020763198, "power_t": 0.5, "max_iter": 200, "shuffle": true, "random_state": 42, "tol": 0.0001, "verbose": false, "warm_start": false, "momentum": 0.8050419419933234, "nesterovs_momentum": true, "early_stopping": false, "validation_fraction": 0.1, "n_iter_no_change": 10, "weights_init_fun": "random_normal", "weights_init_value": 0.7}
epoch	train_loss(squared)	valid_loss(squared)	valid_accuracy(euclidean)
1	3.2242972643000383	3.0982752246047265	2.1210243819021004
2	2.0078459680895713	1.932925336402029	1.7048368712873618
3	1.7099499079613159	1.6244607964388185	1.559638824904811
4	1.416730905981404	1.2412374984366752	1.33885654972471
5	1.121889162780552	1.046824200178844	1.164172098006697
6	1.1064268783469404	1.0711645660846278	1.2146342608931795
7	1.6380788867651785	1.6689379877209505	1.508679344794119
8	1.8277502944892146	1.9114134238502731	1.555521024370451
9	1.143368440398802	1.3204466235017815	1.2740660882243502
10	0.8115866352376058	0.9123357161950636	0.9969728899509259
11	0.9694854308851424	1.2353380007099466	1.1653750857815268
12	0.7497587908724792	0.9055943495322399	1.01624124410658
13	0.8329312856203654	1.0680314295785625	1.1722307640384855
14	0.9213565443089252	1.0710473157126263	1.2023398770436455
15	0.7115326487938757	0.9150185146348059	1.0497087889702523
16	0.701122843089892	0.9582992690762667	1.0501771025980162
17	0.6959977700710394	0.9059439651377874	1.0332243906493848
18	0.6850062915645988	0.9407486740776037	1.0519149830688337
19	0.6671580237832497	0.8908259254760632	1.011852058165375
20	0.7001652404956756	0.9571722317545102	1.0580324819468911
21	0.6497046102148464	0.8986074513550386	1.002571765068955
22	0.6358065721339441	0.8859374399366748	0.9824141530925592
23	0.6767398416281827	0.9562481871164394	1.0362770617715868
24	0.8246534267316722	1.1303186301700907	1.2011135262162307
25	0.591580063767956	0.8978415755942379	0.9910783804230997
26	0.6675558406104816	0.9475749069910102	1.007445248428837
27	0.5765504870220786	0.897060106155135	0.9947338977032661
28	0.5864288061660052	0.9022332933990439	0.9927852511347315
29	0.577444513532166	0.8800347224820526	0.9824843942841889
30	0.5643552080841594	0.8854097092041591	0.9782730572868952
31	0.5600424417819901	0.8784719133918406	0.9730857458520779
32	0.5600562783241448	0.8721205707992127	0.9695328242367348
33	0.5662011229853311	0.8828859618343212	0.982498723736052
34	0.5600504687439415	0.8733923893878262	0.9758234532485872
35	0.5552140806801449	0.8760330154872434	0.9746908693483075
36	0.5649132972742233	0.8917813052562059	0.9870396656583509
37	0.5542303337336385	0.8727172253110144	0.9734788994841383
38	0.5528796906975433	0.8759535045861693	0.9719581924044844
39	0.5588561681734694	0.869402377126976	0.9704552386703791
40	0.5567322139756354	0.8670327624390595	0.9695067206655348
41	0.5552832185640558	0.8765155382026038	0.9775255208884872
42	0.5513630477142886	0.8700149093717414	0.9699959712610948
43	0.5529976310637675	0.8715940084040923	0.9696029200949862
44	0.5565930575236435	0.8812042480045469	0.9788432786831555
45	0.5507636095138333	0.8718778876575901	0.9694590383344698
46	0.5521491965853552	0.8709458628638175	0.968856695332605
47	0.5509159712088727	0.8697825999920651	0.9686044925454381
48	0.5512131193436879	0.8696402576356248	0.9690106186071892
49	0.5516921046012153	0.871196615600394	0.9714495037264286
50	0.5518825190995063	0.8699866156646585	0.9703567179582714
51	0.5522251223473079	0.8700183009840419	0.9706761241348895
52	0.5527838860752271	0.8697064650909556	0.9710791266299243
53	0.5532597496232754	0.8690665211566962	0.9709046314874422
54	0.5541413796258411	0.8691402359428045	0.9720548253953437
55	0.5550302148447742	0.8692421093537708	0.9730889844577614
